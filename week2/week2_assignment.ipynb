{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS4p5KbEfwECQeGSSs/F7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-invic/Architecting-LLMs-WiDS/blob/main/week2/week2_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E01"
      ],
      "metadata": {
        "id": "p5tYICKNEcB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the initial setup\n",
        "# I have used the names.txt file of Andrew Karpathy for the trigram model training and making\n",
        "# yes, the trigram model improves significantly ove the bigram model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
        "stoi['.']=0\n",
        "itos = {i: s for s, i in stoi.items()}\n",
        "\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "    chs = ['.', '.'] +list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        ix3 = stoi[ch3]\n",
        "        input_id = ix1*27+ix2\n",
        "        xs.append(input_id)\n",
        "        ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "print(f'Dataset built. {num} examples')\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((729, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "cHyuEHK8EdbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actual model\n",
        "for l in range(1000):\n",
        "    # forward pass\n",
        "    xenc = F.one_hot(xs, num_classes=729).float()\n",
        "    logits = xenc@W\n",
        "    # can use simply logits = W[xs]\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims = True)\n",
        "    # loss(nll)\n",
        "    loss = -probs[torch.arange(num), ys].log().mean()\n",
        "    # backward pass\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "    W.data += -50*W.grad\n",
        "print(f'Step {l}: Loss is {loss.item()}')"
      ],
      "metadata": {
        "id": "pYzZQhzwFXOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating names\n",
        "g_sample = torch.Generator().manual_seed(2147483647)\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    ix = 0\n",
        "    context_idx = 0\n",
        "    while True:\n",
        "        logits = W[context_idx]\n",
        "        probs = F.softmax(logits, dim=0)\n",
        "        ix_next = torch.multinomial(probs, num_samples=1, generator = g_sample).item()\n",
        "        if ix_next == 0:\n",
        "            break\n",
        "        out.append(itos[ix_next])\n",
        "        context_idx = (context_idx) % 27 *27 + ix_next\n",
        "    print(''.join(out))"
      ],
      "metadata": {
        "id": "VIkJfhtdGHfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E02"
      ],
      "metadata": {
        "id": "QlS-6jZJHRbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when we split the data, we observe the generalization gap\n",
        "# the loss of both the training and the dev appear to be similar, with the loss for dev being really slightly greator than training\n",
        "# the test loss is close to the cev loss\n",
        "import random\n",
        "def build_dataset(words_list):\n",
        "    X, Y = [], []\n",
        "    for w in words_list:\n",
        "        chs = ['.', '.'] + list(w) + ['.']\n",
        "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "            ix1 = stoi[ch1]\n",
        "            ix2 = stoi[ch2]\n",
        "            ix3 = stoi[ch3]\n",
        "            X.append(ix1*27+ix2)\n",
        "            Y.append(ix3)\n",
        "    X = torch.tensor(X)\n",
        "    Y = torch.tensor(Y)\n",
        "    print(f'Dataset shape: {X.shape}')\n",
        "    return X, Y\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "print(\"Training Set:\")\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "print(\"Dev(Validation) Set:\")\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "print(\"Test Set:\")\n",
        "Xte, Yte = build_dataset(words[n2:])"
      ],
      "metadata": {
        "id": "0zZ5oxeTG5A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((729, 27), generator=g)\n",
        "W.requires_grad = True\n",
        "for k in range(1000):\n",
        "  logits = W[Xtr]\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  loss = -probs[torch.arange(len(Xtr)), Ytr].log().mean()\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  W.data += -50 * W.grad\n",
        "print(f'Final training loss (Xtr): {loss.item()}')\n",
        "with torch .no_grad():\n",
        "    logits = W[Xdev]\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims = True)\n",
        "    dev_loss = -probs[torch.arange(len(Xdev)), Ydev].log().mean()\n",
        "print(f'Validation(dev) loss (Xdev) : {dev_loss.item()}')\n",
        "with torch.no_grad():\n",
        "    logits = W[Xte]\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims=True)\n",
        "    test_loss = -probs[torch.arange(len(Xte)), Yte].log().mean()\n",
        "print(f'Test set loss (Xte): {test_loss.item()}')"
      ],
      "metadata": {
        "id": "VPL38zCLHkCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E03"
      ],
      "metadata": {
        "id": "w4F6GLfpIJ4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As smoothing increases, train loss increases (underfitting). Dev loss goes down initially, then rises again.\n",
        "\n",
        "regularization_strengths = [0.1, 0.01, 0.001, 0.0]\n",
        "results = {}\n",
        "best_loss = float('inf')\n",
        "best_reg = None\n",
        "best_W = None\n",
        "for reg in regularization_strengths:\n",
        "    g = torch.Generator().manual_seed(2147483647)\n",
        "    W_temp = torch.randn((729, 27), generator=g, requires_grad=True)\n",
        "    for k in range(500):\n",
        "        logits = W_temp[Xtr]\n",
        "        loss = F.cross_entropy(logits, Ytr) + reg * (W_temp**2).mean()\n",
        "        W_temp.grad = None\n",
        "        loss.backward()\n",
        "        W_temp.data += -50 * W_temp.grad\n",
        "    with torch.no_grad():\n",
        "        logits_dev = W_temp[Xdev]\n",
        "        dev_loss = F.cross_entropy(logits_dev, Ydev).item()\n",
        "        results[reg] = dev_loss\n",
        "        print(f\"Reg Strength {reg}: Dev Loss = {dev_loss:.4f}\")\n",
        "        if dev_loss < best_loss:\n",
        "            best_loss = dev_loss\n",
        "            best_reg = reg\n",
        "            best_W = W_temp.clone()\n",
        "print(f\"Best smoothing strength: {best_reg}\")\n",
        "with torch.no_grad():\n",
        "    logits_test = best_W[Xte]\n",
        "    test_loss = F.cross_entropy(logits_test, Yte).item()\n",
        "print(f\"Final test loss (Xte) using reg={best_reg}: {test_loss:.4f}\")\n",
        "# achieved a test loss of approx 2.26, which is very close to the Dev Loss."
      ],
      "metadata": {
        "id": "B2roID52IKcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E04"
      ],
      "metadata": {
        "id": "5Z2s-OQfIN44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# yes we can delete the use of F.one_hot by simply indexing the rows of W.\n",
        "# the code is as follows-\n",
        "for l in range(500):\n",
        "    # forward pass\n",
        "    # xenc = F.one_hot(xs, num_classes=729).float()\n",
        "    # logits = xenc@W\n",
        "    logits = W[xs]\n",
        "    counts = logits.exp()\n",
        "    probs = counts / counts.sum(1, keepdims = True)\n",
        "    # loss(nll)\n",
        "    loss = -probs[torch.arange(num), ys].log().mean()\n",
        "    # backward pass\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "    W.data += -50*W.grad\n",
        "print(f'Step {l}: Loss is {loss.item()}')"
      ],
      "metadata": {
        "id": "iadG-ADJIQL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E05"
      ],
      "metadata": {
        "id": "vutJfgHvIv3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using F.cross_entropy\n",
        "# we prefer to use the F.cross_entropyas a replacement for log liklihood\n",
        "# it combines all the math done manually and directly give the final negative log liklihood\n",
        "# it is simple, efficient and has the solution for the raw probability of 0\n",
        "import torch.nn.functional as F\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((729,27), generator =g, requires_grad = True)\n",
        "for l in range(500):\n",
        "    logits = W[Xtr]\n",
        "    loss = F.cross_entropy(logits, Ytr) + 0.01*(W**2).mean()\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "    W.data += -50*W.grad\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "g_sample = torch.Generator().manual_seed(2147483647)\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    ix = 0\n",
        "    context_idx = 0\n",
        "    while True:\n",
        "        logits = W[context_idx]\n",
        "        probs = F.softmax(logits, dim=0)\n",
        "        ix_next = torch.multinomial(probs, num_samples=1, generator = g_sample).item()\n",
        "        if ix_next == 0:\n",
        "            break\n",
        "        out.append(itos[ix_next])\n",
        "        context_idx = (context_idx) % 27 *27 + ix_next\n",
        "    print(''.join(out))"
      ],
      "metadata": {
        "id": "uIfL4HYPIyPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E06"
      ],
      "metadata": {
        "id": "ejVXNTV-YOFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Car name generator trigram model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "car_brands = [\n",
        "    'sierra', 'estate', 'sumo', 'safari', 'indica', 'indigo', 'nano', 'aria',\n",
        "    'manza', 'venture', 'zest', 'bolt', 'tiago', 'tigor', 'hexa', 'nexon',\n",
        "    'harrier', 'altroz', 'safarinew', 'punch', 'curvv', 'armada', 'commander',\n",
        "    'marshal', 'major', 'legend', 'bolero', 'scorpio', 'thar', 'xylo', 'quanto',\n",
        "    'verito', 'veritovibe', 'nuvosport', 'marazzo', 'scorpion', 'tharroxx',\n",
        "    'omni', 'gypsy', 'zen', 'esteem', 'balenosedan', 'wagonr', 'alto', 'versa',\n",
        "    'swift', 'zenestilo', 'dzire', 'astar', 'ritz', 'eeco', 'kizashi', 'ertiga',\n",
        "    'celerio', 'ciaz', 'scross', 'balenohatchback', 'vitarabrezza', 'ignis',\n",
        "    'spresso', 'grandvitara', 'fronx', 'jimny', 'invicto', 'evitara', 'landmaster',\n",
        "    'ambassador', 'contessa', 'trekka', 'veer', 'trax', 'gama', 'cruiser', 'toofan',\n",
        "    'gurkha', 'one', 'citiline', 'traveller', 'urbania', 'padmini', 'rio', 'qute',\n",
        "    'defy', 'veer', 'shul', 'ekonk', 'testarossa', 'enzo', 'california', 'ff',\n",
        "    'laferrari', 'portofino', 'roma', 'purosangue', 'miura', 'espada', 'islero',\n",
        "    'jarama', 'urraco', 'countach', 'silhouette', 'jalpa', 'diablo', 'murcilago',\n",
        "    'gallardo', 'reventn', 'aventador', 'sestoelemento', 'veneno', 'huracn',\n",
        "    'centenario', 'urus', 'revuelto', 'temerario', 'senna', 'speedtail', 'gt',\n",
        "    'elva', 'artura', 'solusgt', 'gts', 'veyron', 'chiron', 'divo', 'centodieci',\n",
        "    'bolide', 'mistral', 'tourbillon', 'cc', 'ccr', 'ccx', 'ccxr', 'trevita',\n",
        "    'agera', 'agerar', 'ageras', 'regera', 'jesko', 'gemera', 'zonda', 'huayra',\n",
        "    'utopia', 'imola', 'boxster', 'cayman', 'cayenne', 'carreragt', 'panamera',\n",
        "    'macan', 'taycan', 'silverghost', 'phantom', 'twenty', 'wraith', 'silverwraith',\n",
        "    'silverdawn', 'silvercloud', 'silvershadow', 'corniche', 'camargue',\n",
        "    'silverspirit', 'silverspur', 'silverseraph', 'ghost', 'wraithmodern', 'dawn',\n",
        "    'cullinan', 'spectre', 'droptail', 'speedsix', 'markv', 'markvi', 'rtype',\n",
        "    'mulsanne', 'eight', 'turbor', 'continentalr', 'continentalt', 'azure',\n",
        "    'arnage', 'brooklands', 'continentalgt', 'flyingspur', 'bentayga', 'bacalar',\n",
        "    'batur', 'coalscuttle', 'dbmkiii', 'dbs', 'lagonda', 'virage', 'rapide',\n",
        "    'cygnet', 'vulcan', 'dbssuperleggera', 'dbx', 'valkyrie', 'valhalla',\n",
        "    'valiant', 'sebring', 'mistral', 'quattroporte', 'mexico', 'ghibli', 'indy',\n",
        "    'bora', 'merak', 'khamsin', 'kyalami', 'biturbo', 'karif', 'shamal', 'coup',\n",
        "    'spyder', 'granturismo', 'grancabrio', 'levante', 'grecale', 'granturismonew',\n",
        "    'seven', 'elite', 'elan', 'cortina', 'europa', 'clat', 'esprit', 'excel',\n",
        "    'elise', 'exige', 'evora', 'evija', 'emira', 'eletre', 'emeya', 'conceptone',\n",
        "    'nevera', 'zeppelin', 'landaulet', 'exelero'\n",
        "]\n",
        "chars_cars = sorted(list(set(''.join(car_brands))))\n",
        "stoi_cars = {s:i+1 for i,s in enumerate(chars_cars)}\n",
        "stoi_cars['.'] = 0\n",
        "itos_cars = {i:s for s,i in stoi_cars.items()}\n",
        "num_classes_cars = len(stoi_cars)\n",
        "X_cars, Y_cars = [], []\n",
        "for w in car_brands:\n",
        "    chs = ['.', '.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1 = stoi_cars[ch1]; ix2 = stoi_cars[ch2]; ix3 = stoi_cars[ch3]\n",
        "        X_cars.append(ix1 * num_classes_cars + ix2)\n",
        "        Y_cars.append(ix3)\n",
        "X_cars = torch.tensor(X_cars)\n",
        "Y_cars = torch.tensor(Y_cars)\n",
        "g_cars = torch.Generator().manual_seed(2147483647)\n",
        "W_cars = torch.randn((num_classes_cars*num_classes_cars, num_classes_cars), generator=g_cars, requires_grad=True).to(device)\n",
        "for k in range(1000):\n",
        "    logits = W_cars[X_cars]\n",
        "    loss = F.cross_entropy(logits, Y_cars) + 0.01* (W_cars**2).mean()\n",
        "    W_cars.grad = None\n",
        "    loss.backward()\n",
        "    W_cars.data += -0.5 * W_cars.grad\n",
        "print(f\"Car model trained. Final loss: {loss.item():.4f}\")\n",
        "print(\"YOUR NEW CAR NAMES-\")\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    context_idx = 0\n",
        "    while True:\n",
        "        logits = W_cars[context_idx]\n",
        "        probs = F.softmax(logits, dim=0)\n",
        "        ix_next = torch.multinomial(probs, num_samples=1).item()\n",
        "        if ix_next == 0: break\n",
        "        out.append(itos_cars[ix_next])\n",
        "        context_idx = (context_idx % num_classes_cars) * num_classes_cars + ix_next\n",
        "    print(''.join(out))"
      ],
      "metadata": {
        "id": "BqR_OiPZXofd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}